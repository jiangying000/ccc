#!/usr/bin/env python3
"""
é€†å‘å·¥ç¨‹Claudeçš„tokenè®¡ç®—
åŸºäºJSONLæ–‡ä»¶çš„çœŸå®ç»“æ„
"""

import json
from ccc.extractor import ClaudeContextExtractor

def analyze_jsonl_structure(jsonl_path):
    """åˆ†æJSONLæ–‡ä»¶çš„çœŸå®ç»“æ„å’Œtokenå ç”¨"""
    
    messages = []
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                try:
                    messages.append(json.loads(line))
                except Exception:
                    pass
    
    print("="*60)
    print("JSONLç»“æ„åˆ†æ")
    print("="*60)
    
    # åˆ†æä¸åŒéƒ¨åˆ†çš„å¤§å°
    total_chars = 0
    text_content_chars = 0
    metadata_chars = 0
    structure_chars = 0
    
    for msg in messages:
        # å®Œæ•´æ¶ˆæ¯çš„JSONå¤§å°
        msg_json = json.dumps(msg, ensure_ascii=False, separators=(',', ':'))
        total_chars += len(msg_json)
        
        # æå–çº¯æ–‡æœ¬å†…å®¹
        text = ""
        if 'message' in msg and isinstance(msg['message'], dict):
            content = msg['message'].get('content', [])
            if isinstance(content, list):
                for item in content:
                    if isinstance(item, dict) and item.get('type') == 'text':
                        text += item.get('text', '')
            elif isinstance(content, str):
                text = content
        
        text_content_chars += len(text)
        
        # å…ƒæ•°æ®éƒ¨åˆ†ï¼ˆä¸åŒ…æ‹¬messageå†…å®¹ï¼‰
        metadata = {
            'parentUuid': msg.get('parentUuid'),
            'isSidechain': msg.get('isSidechain'),
            'userType': msg.get('userType'),
            'cwd': msg.get('cwd'),
            'sessionId': msg.get('sessionId'),
            'version': msg.get('version'),
            'gitBranch': msg.get('gitBranch'),
            'type': msg.get('type'),
            'uuid': msg.get('uuid'),
            'timestamp': msg.get('timestamp'),
        }
        metadata_json = json.dumps(metadata, ensure_ascii=False, separators=(',', ':'))
        metadata_chars += len(metadata_json)
    
    # ç»“æ„å¼€é”€ = æ€»å¤§å° - çº¯æ–‡æœ¬
    structure_chars = total_chars - text_content_chars
    
    print(f"\nğŸ“ æ–‡ä»¶: {jsonl_path.name}")
    print(f"   æ¶ˆæ¯æ•°: {len(messages)}")
    print(f"   æ–‡ä»¶å¤§å°: {jsonl_path.stat().st_size / 1024:.1f} KB")
    
    print("\nğŸ“Š å­—ç¬¦ç»Ÿè®¡:")
    print(f"   æ€»JSONå­—ç¬¦: {total_chars:,}")
    print(f"   çº¯æ–‡æœ¬å†…å®¹: {text_content_chars:,} ({text_content_chars*100//total_chars}%)")
    print(f"   å…ƒæ•°æ®: {metadata_chars:,} ({metadata_chars*100//total_chars}%)")
    print(f"   JSONç»“æ„: {structure_chars:,} ({structure_chars*100//total_chars}%)")
    
    # Tokenä¼°ç®—ï¼ˆåŸºäºClaudeçš„tokenizerç‰¹ç‚¹ï¼‰
    # Claudeå¯¹JSONçš„å¤„ç†ï¼š
    # 1. è‹±æ–‡æ–‡æœ¬ï¼šçº¦3.5å­—ç¬¦/token
    # 2. JSONé”®å€¼å¯¹ï¼šçº¦2.5å­—ç¬¦/tokenï¼ˆå› ä¸ºæœ‰å¾ˆå¤šçŸ­é”®ï¼‰
    # 3. UUIDç­‰ï¼šçº¦2å­—ç¬¦/tokenï¼ˆå¯†é›†å­—ç¬¦ï¼‰
    
    text_tokens = text_content_chars / 3.5
    structure_tokens = structure_chars / 2.5
    total_tokens_estimate = text_tokens + structure_tokens
    
    print("\nğŸ¯ Tokenä¼°ç®—:")
    print(f"   æ–‡æœ¬tokens: {int(text_tokens):,}")
    print(f"   ç»“æ„tokens: {int(structure_tokens):,}")
    print(f"   æ€»è®¡: {int(total_tokens_estimate):,}")
    
    # åŸºäºæ–‡ä»¶å¤§å°çš„ç»éªŒå…¬å¼
    # æ‚¨æåˆ°çš„å®é™…ï¼š1.8MB â‰ˆ 139k tokens
    # æ¯”ä¾‹ï¼š1KB â‰ˆ 77 tokens
    file_size_kb = jsonl_path.stat().st_size / 1024
    empirical_tokens = int(file_size_kb * 77)
    
    print("\nğŸ“ ç»éªŒå…¬å¼:")
    print(f"   åŸºäºæ–‡ä»¶å¤§å°(1KBâ‰ˆ77tokens): {empirical_tokens:,}")
    
    return {
        'messages': len(messages),
        'total_chars': total_chars,
        'text_chars': text_content_chars,
        'structure_chars': structure_chars,
        'estimated_tokens': int(total_tokens_estimate),
        'empirical_tokens': empirical_tokens
    }

def compare_with_ccc():
    """å¯¹æ¯”CCCçš„è®¡ç®—å’Œå®é™…"""
    
    extractor = ClaudeContextExtractor()
    sessions = extractor.find_claude_sessions()
    
    if len(sessions) < 3:
        print("æ²¡æœ‰è¶³å¤Ÿçš„ä¼šè¯")
        return
    
    # åˆ†æç¬¬3ä¸ªä¼šè¯ï¼ˆæ‚¨æåˆ°çš„ï¼‰
    session_path = sessions[2]
    
    # CCCçš„è®¡ç®—
    info = extractor.get_session_info(session_path)
    ccc_tokens = info['tokens']
    
    # æˆ‘ä»¬çš„åˆ†æ
    analysis = analyze_jsonl_structure(session_path)
    
    print("\n" + "="*60)
    print("å¯¹æ¯”åˆ†æ")
    print("="*60)
    
    print(f"\nCCCæ˜¾ç¤º: {ccc_tokens:,} tokens")
    print(f"ç»“æ„åˆ†æä¼°ç®—: {analysis['estimated_tokens']:,} tokens")
    print(f"ç»éªŒå…¬å¼: {analysis['empirical_tokens']:,} tokens")
    print("æ‚¨æåˆ°çš„å®é™…: 139,000 tokens")
    
    print("\nè¯¯å·®åˆ†æ:")
    print(f"CCCè¯¯å·®: {abs(139000 - ccc_tokens):,} ({abs(139000 - ccc_tokens)*100//139000}%)")
    print(f"ç»“æ„åˆ†æè¯¯å·®: {abs(139000 - analysis['estimated_tokens']):,} ({abs(139000 - analysis['estimated_tokens'])*100//139000}%)")
    print(f"ç»éªŒå…¬å¼è¯¯å·®: {abs(139000 - analysis['empirical_tokens']):,} ({abs(139000 - analysis['empirical_tokens'])*100//139000}%)")
    
    print("\nğŸ’¡ ç»“è®º:")
    print("CCCåªè®¡ç®—äº†message.content.textéƒ¨åˆ†ï¼Œ")
    print("å¿½ç•¥äº†JSONLçš„æ‰€æœ‰å…ƒæ•°æ®å’Œç»“æ„å¼€é”€ã€‚")
    print("å®é™…ä¸ŠClaudeéœ€è¦å¤„ç†å®Œæ•´çš„JSONLï¼ŒåŒ…æ‹¬ï¼š")
    print("- parentUuid, sessionId, timestampç­‰å…ƒæ•°æ®")
    print("- messageçš„roleå’Œcontentæ•°ç»„ç»“æ„")
    print("- typeå­—æ®µå’Œå…¶ä»–æ§åˆ¶ä¿¡æ¯")

if __name__ == "__main__":
    compare_with_ccc()